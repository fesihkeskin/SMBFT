#!/usr/bin/env python3
# gather_ablation_results.py

"""
Gathers Test Overall Accuracy (OA) results from ablation study JSON files
and prints formatted tables for a research paper.

This script reads the 'train_history_... .json' files generated by
'ablate.py' and 'train.py'. Note that these specific JSON files only
contain the Test OA metric. Average Accuracy (AA) and Kappa (κ) are
calculated by test.py but not saved in train_history.json.

It calculates the mean and standard deviation of Test OA over multiple
seeds for each experiment and prints pandas and LaTeX tables.


Usage:
python gather_ablation_results.py --dataset Houston13
"""

import json
import sys
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

# --- CONFIGURATION (Must match ablate.py) ---
# project-root on PYTHONPATH
PROJECT_ROOT = Path(__file__).resolve().parents[0]
sys.path.insert(0, str(PROJECT_ROOT))

print(f"INFO: Project root set to '{PROJECT_ROOT}'")
ABLATION_ROOT = PROJECT_ROOT / "reports" / "ablation_studies"

# Define multiple seeds for robust evaluation
SEEDS = [242, 343, 454]

# This configuration defines the *order* and *display names* of the rows
# in the final table. Keys must match the experiment names in ablate.py.
ABLATION_CONFIG_ORDERED = {
    "01_baseline": "Baseline (Full Model)",
    "02_no_augmentations": "Baseline - All Augmentations",
    "03_no_sample_mixing": "Baseline - (MixUp + CutMix)",
    "04_no_spectral_augmentations": "Baseline - Spectral Augmentations",
    "05_fusion_cross": "Baseline (Fusion: Query Pooling)",
    "06_pca_15_components": "Baseline + PCA (15 Components)",
    "07_pca_30_components": "Baseline + PCA (30 Components)",
    "08_pca_50_components": "Baseline + PCA (50 Components)",
    "09_no_spectral_masking": "Baseline - Spectral Masking (SSL)",
}

# --- END CONFIGURATION ---

def parse_json_result(json_path: Path) -> Optional[float]:
    """
    Parses a single train_history.json file and extracts the test_OA metric.
    Returns None if the file is missing, corrupt, or lacks the metric.
    """
    if not json_path.exists():
        print(f"Warning: Result file not found: {json_path}")
        return None

    try:
        with open(json_path) as f:
            data = json.load(f)

        # .get() is safer and returns None if the key doesn't exist
        oa = data.get("test_OA")

        # Handle cases where training failed and metrics are None or invalid (-1.0)
        if oa is None or oa < 0:
            print(f"Warning: Missing or invalid 'test_OA' in {json_path}. Skipping.")
            return None

        return float(oa)

    except json.JSONDecodeError:
        print(f"Warning: Could not parse (corrupt) JSON: {json_path}")
        return None
    except Exception as e:
        print(f"Warning: Error reading {json_path}: {e}")
        return None

def format_metric(scores: List[float]) -> str:
    """
    Calculates mean/std and formats them as a string "Mean ± Std".
    Returns "N/A" if no valid scores are provided.
    """
    if not scores:
        return "N/A"

    # Multiply by 100 for percentages
    mean = np.mean(scores) * 100
    std = np.std(scores) * 100

    return f"{mean:.2f} ± {std:.2f}"

def gather_main(dataset: str):
    """
    Main function to gather and print results.
    """
    print(f"Gathering ablation results for dataset: {dataset}")
    print(f"Reading from root directory: {ABLATION_ROOT / dataset}")
    print(f"Using seeds: {SEEDS}\n")

    table_data = []

    for exp_name, display_name in ABLATION_CONFIG_ORDERED.items():
        oa_scores = []

        for seed in SEEDS:
            # Construct the path to the specific JSON file for this experiment and seed
            json_path = ABLATION_ROOT / dataset / exp_name / f"seed_{seed}" / f"train_history_{dataset}_seed{seed}.json"

            oa = parse_json_result(json_path)

            if oa is not None:
                oa_scores.append(oa)

        # Format the results for the table
        table_data.append({
            "Ablation Study": display_name,
            "Test OA (%)": format_metric(oa_scores),
            "Runs Completed": len(oa_scores) # Show how many runs succeeded per experiment
        })

    # --- Print Tables ---
    if not table_data:
        print("No results found. Did the ablation studies run correctly and finish?")
        return

    # Create and print Pandas DataFrame
    df = pd.DataFrame(table_data)
    df.set_index("Ablation Study", inplace=True)

    print("\n" + "="*80)
    print(f"Ablation Study Results Table for Dataset: {dataset}")
    print("(Metrics derived from train_history_*.json files)")
    print("Note: AA and Kappa are not available in these specific source files.")
    print("="*80)
    print(df.to_string())
    print("\n" + "="*80)

    # Print LaTeX-formatted table
    print("Ablation Study Results Table (LaTeX)")
    print("="*80)
    # Get column format: l (left-aligned) for index, c (center) for metrics
    col_format = "l|" + "c" * (len(df.columns))
    # Escape the '%' sign for LaTeX
    df_latex = df.rename(columns={"Test OA (%)": r"Test OA (\%)"})
    latex_table = df_latex.to_latex(column_format=col_format, bold_rows=True, escape=False)
    print(latex_table)
    print("="*80)

    # Save LaTeX table to file
    out_dir = PROJECT_ROOT / "reports" / "results"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"ablation_results_{dataset}.tex"
    with open(out_path, "w") as f:
        f.write(latex_table)
    print(f"✅ LaTeX table saved to {out_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Gather and tabulate ablation study results (OA only) from train_history JSON files.")
    parser.add_argument(
        "--dataset",
        type=str,
        required=True,
        help="Name of the dataset to gather results for (Houston13, Pavia_University, Salinas)."
    )
    args = parser.parse_args()

    # Check if the dataset ablation directory exists
    dataset_dir = ABLATION_ROOT / args.dataset
    if not dataset_dir.is_dir():
        print(f"ERROR: Ablation directory not found for dataset '{args.dataset}' at: {dataset_dir}")
        print("Please ensure 'ablate.py' has been run for this dataset.")
        exit(1)

    gather_main(args.dataset)

